{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data skripsi/dataset_ulasan_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacers = {'sy':'saya','gak':'enggak','ssah':'susah','bgt':'banget','skrng':'sekarang','yg':'yang','gw':'gue',\n",
    "             'baguuus':'bagus','mmbantu':'membantu', 'parahg':'parah'}\n",
    "\n",
    "\n",
    "def word_normalization(doc):\n",
    "    for key in replacers.keys():\n",
    "        doc = doc.replace(key,replacers[key])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_normalization2(sentence, dictionary):\n",
    "    return \" \".join([dictionary.get(w,w) for w in sentence.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data\n",
    "stop = set(stopwords.words('indonesian'))\n",
    "exclude = set(string.punctuation + '1234567890')\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def preprocess1(text):\n",
    "    case_folded = text.lower()\n",
    "    symbol_removed = ''.join(ch for ch in case_folded if ch not in exclude)\n",
    "    word_normalized = words_normalization2(symbol_removed, replacers)\n",
    "    tokenized = word_tokenize(word_normalized)\n",
    "    #stop_free = ' '.join([word for word in text.lower().split() if word not in stop])\n",
    "    #stemmed = ' '.join([stemmer.stem(word) for word in num_and_punc_free.split()])\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed1_ulasan'] = df['Ulasan'].apply(preprocess1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "ct = CRFTagger()\n",
    "ct.set_model_file('all_indo_man_tag_corpus_model.crf.tagger')\n",
    "\n",
    "pos_ulasan = ct.tag_sents(df.preprocessed1_ulasan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_data = list()\n",
    "for i,text in enumerate(pos_ulasan, start=0):\n",
    "    temp = list()\n",
    "    for term in text:\n",
    "        if term[1] == 'VB' or term[1] == 'NN' or term[1] == 'JJ' or term[1] == 'RB' or term[1] == 'NEG' or term[1] == 'NNP':\n",
    "        #if term[1] == 'NNP' or term[1] == 'NN':\n",
    "            temp.append(term[0])\n",
    "    noun_data.append(temp)\n",
    "\n",
    "df['pos_ulasan'] = df['Ulasan']\n",
    "\n",
    "for i in range(len(df['pos_ulasan'])):\n",
    "    df['pos_ulasan'][i] = noun_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def neg_handler(doc):\n",
    " #   for key in neg_word.keys():\n",
    "  #      doc = doc.replace(key,neg_word[key])\n",
    "   # return doc\n",
    "\n",
    "def preprocess2(text):\n",
    "    text = TreebankWordDetokenizer().detokenize(text)\n",
    "    stop_removed = ' '.join([word for word in text.split() if word not in stop])\n",
    "    stemmed = ' '.join([stemmer.stem(word) for word in stop_removed.split()])\n",
    "    tokenized = word_tokenize(stemmed)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed2_ulasan'] = df['pos_ulasan'].apply(preprocess2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [aplikasi, sangat, membantu, saat, pandemi, ap...\n",
       "1                  [memesan, perjalanan, lebih, mudah]\n",
       "2    [kai, smakin, bagus, pelayananya, krena, situa...\n",
       "3              [kedepannya, lebih, menyenangkan, lagi]\n",
       "4                                             [merasa]\n",
       "Name: pos_ulasan, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pos_ulasan'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tokens):\n",
    "    return TreebankWordDetokenizer().detokenize(tokens)\n",
    "\n",
    "df['preprocessed3_ulasan'] = df['pos_ulasan'].apply(detokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed3_ulasan'].replace(\"\", float(\"NaN\"), inplace=True)\n",
    "df.dropna(subset=['preprocessed3_ulasan'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Ulasan'] = df['preprocessed3_ulasan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnability = df.Learnability\n",
    "efficiency = df.Efficiency\n",
    "memorability = df.Memorability\n",
    "errors = df.Errors\n",
    "satisfaction = df.Satisfaction\n",
    "ulasan = df.Ulasan\n",
    "new_df = pd.concat([learnability,efficiency,memorability,errors,satisfaction,ulasan], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('Data skripsi/preprocessed_data_sentimen.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
