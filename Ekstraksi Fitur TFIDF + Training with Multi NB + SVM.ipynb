{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  #Read Data\n",
    "\n",
    "def read_data(file_dir):\n",
    "    df = pd.read_csv(file_dir)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_data(df, col_name):\n",
    "    one = 0\n",
    "    zero = 0\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    for label in df[col_name]:\n",
    "        if label == 1:\n",
    "            one += 1\n",
    "        else:\n",
    "            zero += 1\n",
    "    ax.bar(0.00, [one], color = 'g', width = 0.25)\n",
    "    ax.bar(0.25, [zero], color = 'r', width = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import word_tokenize\n",
    "\n",
    "replacers = {'sy':'saya','gak':'enggak','ssah':'susah','bgt':'banget','skrng':'sekarang','yg':'yang','gw':'gue',\n",
    "             'baguuus':'bagus','mmbantu':'membantu', 'parahg':'parah'}\n",
    "\n",
    "def words_normalization(sentence, dictionary):\n",
    "    return \" \".join([dictionary.get(w,w) for w in sentence.split()])\n",
    "\n",
    "def preprocess1(text):\n",
    "    case_folded = text.lower()\n",
    "    exclude = set(string.punctuation + '1234567890')\n",
    "    symbol_removed = ''.join(ch for ch in case_folded if ch not in exclude)\n",
    "    word_normalized = words_normalization(symbol_removed, replacers)\n",
    "    tokenized = word_tokenize(word_normalized)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "\n",
    "def pos_tagging(data_ulasan):\n",
    "    ct = CRFTagger()\n",
    "    ct.set_model_file('all_indo_man_tag_corpus_model.crf.tagger')\n",
    "\n",
    "    pos_ulasan = ct.tag_sents(data_ulasan)\n",
    "\n",
    "    noun_data = list()\n",
    "    for i,text in enumerate(pos_ulasan, start=0):\n",
    "        temp = list()\n",
    "        for term in text:\n",
    "            if term[1] == 'VB' or term[1] == 'NN' or term[1] == 'JJ' or term[1] == 'RB':\n",
    "                temp.append(term[0])\n",
    "        noun_data.append(temp)\n",
    "\n",
    "    pos_ulasan = data_ulasan\n",
    "\n",
    "    for i in range(len(pos_ulasan)):\n",
    "        pos_ulasan[i] = noun_data[i]\n",
    "    \n",
    "    return pos_ulasan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "def preprocess2(text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    stop = set(stopwords.words('indonesian'))\n",
    "    text = TreebankWordDetokenizer().detokenize(text)\n",
    "    \n",
    "    stop_removed = ' '.join([word for word in text.split() if word not in stop])\n",
    "    stemmed = ' '.join([stemmer.stem(word) for word in stop_removed.split()])\n",
    "    tokenized = word_tokenize(stemmed)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "def apply_oversampling(X, y):\n",
    "    print('Original dataset shape %s' % Counter(y))\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    print('Resampled dataset shape %s' % Counter(y_res))\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert label to negative 0 or positive 1\n",
    "import numpy\n",
    "\n",
    "def convert_sentiment_label(df, col_name):\n",
    "    index_nan = list()\n",
    "    df2 = df[['Ulasan', col_name]].copy()\n",
    "    df2[col_name].replace(str(\"pos\"), 1, inplace=True)\n",
    "    df2[col_name].replace(str(\"neg\"), 0, inplace=True)\n",
    "    df2[col_name].replace(\"\", float(\"NaN\"), inplace=True)\n",
    "    df2.dropna(subset=[col_name], inplace=True)\n",
    "    df2[col_name] = df2[col_name].astype(numpy.int64)\n",
    "    \n",
    "    return df2\n",
    "\n",
    "def convert_aspect_label(df, col_name):\n",
    "    index_nan = list()\n",
    "    df2 = df[['Ulasan', col_name]].copy()\n",
    "    df2[col_name].replace(float(\"NaN\"), 0, inplace=True)\n",
    "    df2[col_name].replace(str(\"pos\"), 1, inplace=True)\n",
    "    df2[col_name].replace(str(\"neg\"), 1, inplace=True)\n",
    "    df2[col_name] = df2[col_name].astype(numpy.int64)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df, col_name, test_size):\n",
    "    words = df.Ulasan.tolist()\n",
    "    label = df[col_name].tolist()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(words, label, test_size=test_size, random_state=10)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/53294482/how-to-get-tf-idf-scores-for-the-words\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_vectorizer(x_train, x_test):\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n",
    "    vecx_train = vectorizer.fit_transform(x_train).toarray()\n",
    "    vecx_test = vectorizer.transform(x_test).toarray()\n",
    "    return vecx_train, vecx_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from time import time\n",
    "\n",
    "def train_data_nb(x_train, y_train):\n",
    "    t0 = time()\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x_train, y_train)\n",
    "    print(f\"\\nTraining time: {round(time()-t0, 3)}s\")\n",
    "    t0 = time()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def train_data_svm(x_train, y_train):\n",
    "    t0 = time()\n",
    "    model = svm.SVC(kernel='linear')\n",
    "    model.fit(x_train, y_train)\n",
    "    print(f\"\\nTraining time: {round(time()-t0, 3)}s\")\n",
    "    t0 = time()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(text, model):\n",
    "    data = [text]\n",
    "    vec_data = vectorizer.transform(data).toarray()\n",
    "    return model.predict(vec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, vectorized_test_data, label):\n",
    "    x_test = vectorized_test_data\n",
    "    y_pred = model.predict(x_test)\n",
    "    print(classification_report(label, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search_cv(X_train, y_train):\n",
    "    # defining parameter range\n",
    "    param_grid = {'C': [0.1, 1, 10], \n",
    "                  'gamma': [1, 0.1, 0.01],\n",
    "                  'kernel': ['linear']} \n",
    "  \n",
    "    grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "    # fitting the model for grid search\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(grid.best_params_)\n",
    "    print(grid.best_estimator_)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Program!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = read_data(\"Data skripsi/fix hasil label/aspek_labeling_1.csv\")\n",
    "df = read_data(\"Data skripsi/dataset_ulasan_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#aspects: 'Learnability', 'Efficiency', 'Memorability', 'Errors', 'Satisfaction'\n",
    "#Pelabelan dengan 1 dan 0\n",
    "\n",
    "col_name = 'Errors'\n",
    "df_sentimen = convert_sentiment_label(df, col_name)\n",
    "df_aspect = convert_aspect_label(df, col_name) \n",
    "visualize_data(df_sentimen, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data \n",
    "\n",
    "x_train1, x_test1, y_train1, y_test1 = split_data(df_aspect, col_name, test_size= 0.2) #Split Data untuk klasifikasi aspek\n",
    "x_train2, x_test2, y_train2, y_test2 = split_data(df_sentimen, col_name, test_size= 0.2) #Split Data untuk klasifikasi sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing 1\n",
    "\n",
    "x_train1 = [preprocess1(text) for text in x_train1]\n",
    "x_test1 = [preprocess1(text) for text in x_test1]\n",
    "\n",
    "x_train2 = [preprocess1(text) for text in x_train2]\n",
    "x_test2 = [preprocess1(text) for text in x_test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pos Tagging untuk data ulasan klasifikasi sentimen\n",
    "\n",
    "x_train2 = pos_tagging(x_train2)\n",
    "x_test2 = pos_tagging(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess 2\n",
    "\n",
    "x_train1 = [preprocess2(text) for text in x_train1]\n",
    "x_test1 = [preprocess2(text) for text in x_test1]\n",
    "\n",
    "x_train2 = [preprocess2(text) for text in x_train2]\n",
    "x_test2 = [preprocess2(text) for text in x_test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tokens):\n",
    "    return TreebankWordDetokenizer().detokenize(tokens)\n",
    "\n",
    "x_train1 = [detokenize(text) for text in x_train1]\n",
    "x_test1 = [detokenize(text) for text in x_test1]\n",
    "\n",
    "x_train2 = [detokenize(text) for text in x_train2]\n",
    "x_test2 = [detokenize(text) for text in x_test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ekstraksi fitur TF-IDF\n",
    "\n",
    "vx_train1, vx_test1, vectorizer1 = tfidf_vectorizer(x_train1, x_test1) #Untuk klasifikasi aspek\n",
    "vx_train2, vx_test2, vectorizer2 = tfidf_vectorizer(x_train2, x_test2) #Untuk klasifikasi sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({1: 2383, 0: 650})\n",
      "Resampled dataset shape Counter({1: 2383, 0: 2383})\n",
      "----------------------------------------------------\n",
      "Original dataset shape Counter({0: 2349, 1: 48})\n",
      "Resampled dataset shape Counter({0: 2349, 1: 2349})\n"
     ]
    }
   ],
   "source": [
    "#Oversampling\n",
    "\n",
    "vx_train1, y_train1 = apply_oversampling(vx_train1, y_train1) #Oversampling data untuk klasifikasi aspek\n",
    "print('----------------------------------------------------')\n",
    "vx_train2, y_train2 = apply_oversampling(vx_train2, y_train2) ##Oversampling data untuk klasifikasi aspek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 0.087s\n",
      "--------------------------------------------------------\n",
      "\n",
      "Training time: 0.064s\n"
     ]
    }
   ],
   "source": [
    "#Model Training Multi-NB\n",
    "\n",
    "model_nb1 = train_data_nb(vx_train1, y_train1) #Training Model klasifikasi aspek \n",
    "print('--------------------------------------------------------')\n",
    "model_nb2 = train_data_nb(vx_train2, y_train2) #Training Model klasifikasi sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.39      0.34       145\n",
      "           1       0.85      0.79      0.82       614\n",
      "\n",
      "    accuracy                           0.72       759\n",
      "   macro avg       0.58      0.59      0.58       759\n",
      "weighted avg       0.74      0.72      0.73       759\n",
      "\n",
      "None\n",
      "-----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95       590\n",
      "           1       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.90       600\n",
      "   macro avg       0.49      0.46      0.47       600\n",
      "weighted avg       0.97      0.90      0.93       600\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model_nb1, vx_test1, y_test1))\n",
    "print('-----------------------------------------')\n",
    "print(evaluate_model(model_nb2, vx_test2, y_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 53.104s\n",
      "----------------------------------------------------------\n",
      "\n",
      "Training time: 8.909s\n"
     ]
    }
   ],
   "source": [
    "#Model Training SVM\n",
    "\n",
    "model_svm1 = train_data_svm(vx_train1, vx_test1, y_train1, y_test1) #Training Model klasifikasi aspek \n",
    "print('----------------------------------------------------------')\n",
    "model_svm2 = train_data_svm(vx_train2, vx_test2, y_train2, y_test2) #Training Model klasifikasi sentimen \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.33      0.30       145\n",
      "           1       0.83      0.79      0.81       614\n",
      "\n",
      "    accuracy                           0.70       759\n",
      "   macro avg       0.55      0.56      0.55       759\n",
      "weighted avg       0.72      0.70      0.71       759\n",
      "\n",
      "None\n",
      "-----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       590\n",
      "           1       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.98       600\n",
      "   macro avg       0.49      0.50      0.49       600\n",
      "weighted avg       0.97      0.98      0.97       600\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model_svm1, vx_test1, y_test1))\n",
    "print('-----------------------------------------')\n",
    "print(evaluate_model(model_svm2, vx_test2, y_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "[CV 1/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.624 total time=  55.5s\n",
      "[CV 2/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.673 total time=  56.8s\n",
      "[CV 3/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.704 total time=  57.6s\n",
      "[CV 4/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.717 total time=  58.1s\n",
      "[CV 5/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.702 total time=  55.9s\n",
      "[CV 1/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.624 total time=  55.5s\n",
      "[CV 2/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.673 total time=  57.0s\n",
      "[CV 3/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.704 total time=  56.9s\n",
      "[CV 4/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.717 total time=  56.7s\n",
      "[CV 5/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.702 total time=  57.1s\n",
      "[CV 1/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.624 total time=  56.1s\n",
      "[CV 2/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.673 total time=  57.6s\n",
      "[CV 3/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.704 total time=  56.8s\n",
      "[CV 4/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.717 total time=  56.9s\n",
      "[CV 5/5] END ..C=0.1, gamma=0.01, kernel=linear;, score=0.702 total time=  57.8s\n",
      "[CV 1/5] END .......C=1, gamma=1, kernel=linear;, score=0.783 total time=  43.9s\n",
      "[CV 2/5] END .......C=1, gamma=1, kernel=linear;, score=0.808 total time=  45.2s\n",
      "[CV 3/5] END .......C=1, gamma=1, kernel=linear;, score=0.841 total time=  45.9s\n",
      "[CV 4/5] END .......C=1, gamma=1, kernel=linear;, score=0.842 total time=  45.9s\n",
      "[CV 5/5] END .......C=1, gamma=1, kernel=linear;, score=0.852 total time=  46.0s\n",
      "[CV 1/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.783 total time=  43.9s\n",
      "[CV 2/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.808 total time=  45.7s\n",
      "[CV 3/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.841 total time=  50.6s\n",
      "[CV 4/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.842 total time=  47.4s\n",
      "[CV 5/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.852 total time=  46.7s\n",
      "[CV 1/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.783 total time=  44.6s\n",
      "[CV 2/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.808 total time=  45.9s\n",
      "[CV 3/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.841 total time=  46.7s\n",
      "[CV 4/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.842 total time=  46.3s\n",
      "[CV 5/5] END ....C=1, gamma=0.01, kernel=linear;, score=0.852 total time=  46.8s\n",
      "[CV 1/5] END ......C=10, gamma=1, kernel=linear;, score=0.831 total time=  33.8s\n",
      "[CV 2/5] END ......C=10, gamma=1, kernel=linear;, score=0.853 total time=  33.0s\n",
      "[CV 3/5] END ......C=10, gamma=1, kernel=linear;, score=0.871 total time=  50.5s\n",
      "[CV 4/5] END ......C=10, gamma=1, kernel=linear;, score=0.881 total time=  32.8s\n",
      "[CV 5/5] END ......C=10, gamma=1, kernel=linear;, score=0.877 total time=  32.9s\n",
      "[CV 1/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.831 total time=  33.5s\n",
      "[CV 2/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.853 total time=  33.6s\n",
      "[CV 3/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.871 total time=  55.3s\n",
      "[CV 4/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.881 total time=  35.2s\n",
      "[CV 5/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.877 total time=  35.1s\n",
      "[CV 1/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.831 total time=  36.9s\n",
      "[CV 2/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.853 total time=  36.7s\n",
      "[CV 3/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.871 total time=  55.8s\n",
      "[CV 4/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.881 total time=  36.0s\n",
      "[CV 5/5] END ...C=10, gamma=0.01, kernel=linear;, score=0.877 total time=  37.3s\n",
      "{'C': 10, 'gamma': 1, 'kernel': 'linear'}\n",
      "SVC(C=10, gamma=1, kernel='linear')\n"
     ]
    }
   ],
   "source": [
    "#Model Training SVM dengan GridSearchCV\n",
    "\n",
    "grid_svm1 = grid_search_cv(vx_train1, y_train1) #Untuk Model klasifikasi aspek "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_svm2 = grid_search_cv(vx_train2, y_train2) #Untuk Model klasifikasi sentimen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.33      0.29       145\n",
      "           1       0.83      0.78      0.81       614\n",
      "\n",
      "    accuracy                           0.70       759\n",
      "   macro avg       0.55      0.56      0.55       759\n",
      "weighted avg       0.72      0.70      0.71       759\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(grid_svm1, vx_test1, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOurce: https://towardsdatascience.com/training-a-naive-bayes-model-to-identify-the-author-of-an-email-or-document-17dc85fa630a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
