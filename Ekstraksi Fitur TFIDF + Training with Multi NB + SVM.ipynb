{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  #Read Data\n",
    "\n",
    "def read_data(file_dir):\n",
    "    df = pd.read_csv(file_dir)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_data(df, col_name):\n",
    "    one = 0\n",
    "    zero = 0\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    for label in df[col_name]:\n",
    "        if label == 1:\n",
    "            one += 1\n",
    "        else:\n",
    "            zero += 1\n",
    "    ax.bar(0.00, [one], color = 'g', width = 0.25)\n",
    "    ax.bar(0.25, [zero], color = 'r', width = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk import word_tokenize\n",
    "\n",
    "replacers = {'sy':'saya','gak':'enggak','ssah':'susah','bgt':'banget','skrng':'sekarang','yg':'yang','gw':'gue',\n",
    "             'baguuus':'bagus','mmbantu':'membantu', 'parahg':'parah'}\n",
    "\n",
    "def words_normalization(sentence, dictionary):\n",
    "    return \" \".join([dictionary.get(w,w) for w in sentence.split()])\n",
    "\n",
    "def preprocess1(text):\n",
    "    case_folded = text.lower()\n",
    "    exclude = set(string.punctuation + '1234567890')\n",
    "    symbol_removed = ''.join(ch for ch in case_folded if ch not in exclude)\n",
    "    word_normalized = words_normalization(symbol_removed, replacers)\n",
    "    tokenized = word_tokenize(word_normalized)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import CRFTagger\n",
    "\n",
    "def pos_tagging(data_ulasan):\n",
    "    ct = CRFTagger()\n",
    "    ct.set_model_file('all_indo_man_tag_corpus_model.crf.tagger')\n",
    "\n",
    "    pos_ulasan = ct.tag_sents(data_ulasan)\n",
    "\n",
    "    noun_data = list()\n",
    "    for i,text in enumerate(pos_ulasan, start=0):\n",
    "        temp = list()\n",
    "        for term in text:\n",
    "            if term[1] == 'VB' or term[1] == 'NN' or term[1] == 'JJ' or term[1] == 'RB':\n",
    "                temp.append(term[0])\n",
    "        noun_data.append(temp)\n",
    "\n",
    "    pos_ulasan = data_ulasan\n",
    "\n",
    "    for i in range(len(pos_ulasan)):\n",
    "        pos_ulasan[i] = noun_data[i]\n",
    "    \n",
    "    return pos_ulasan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "def preprocess2(text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    stop = set(stopwords.words('indonesian'))\n",
    "    text = TreebankWordDetokenizer().detokenize(text)\n",
    "    \n",
    "    stop_removed = ' '.join([word for word in text.split() if word not in stop])\n",
    "    stemmed = ' '.join([stemmer.stem(word) for word in stop_removed.split()])\n",
    "    tokenized = word_tokenize(stemmed)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "def apply_oversampling(X, y):\n",
    "    print('Original dataset shape %s' % Counter(y))\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "    print('Resampled dataset shape %s' % Counter(y_res))\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert label to negative 0 or positive 1\n",
    "import numpy\n",
    "\n",
    "def convert_sentiment_label(df, col_name):\n",
    "    index_nan = list()\n",
    "    df2 = df[['Ulasan', col_name]].copy()\n",
    "    df2[col_name].replace(str(\"pos\"), 1, inplace=True)\n",
    "    df2[col_name].replace(str(\"neg\"), 0, inplace=True)\n",
    "    df2[col_name].replace(\"\", float(\"NaN\"), inplace=True)\n",
    "    df2.dropna(subset=[col_name], inplace=True)\n",
    "    df2[col_name] = df2[col_name].astype(numpy.int64)\n",
    "    \n",
    "    return df2\n",
    "\n",
    "def convert_aspect_label(df, col_name):\n",
    "    index_nan = list()\n",
    "    df2 = df[['Ulasan', col_name]].copy()\n",
    "    df2[col_name].replace(float(\"NaN\"), 0, inplace=True)\n",
    "    df2[col_name].replace(str(\"pos\"), 1, inplace=True)\n",
    "    df2[col_name].replace(str(\"neg\"), 1, inplace=True)\n",
    "    df2[col_name] = df2[col_name].astype(numpy.int64)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df, col_name, test_size):\n",
    "    words = df.Ulasan.tolist()\n",
    "    label = df[col_name].tolist()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(words, label, test_size=test_size, random_state=10)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/53294482/how-to-get-tf-idf-scores-for-the-words\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_vectorizer(x_train, x_test):\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5)\n",
    "    vecx_train = vectorizer.fit_transform(x_train).toarray()\n",
    "    vecx_test = vectorizer.transform(x_test).toarray()\n",
    "    return vecx_train, vecx_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from time import time\n",
    "\n",
    "def train_data_nb(x_train, x_test, y_train, y_test):\n",
    "    t0 = time()\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x_train, y_train)\n",
    "    print(f\"\\nTraining time: {round(time()-t0, 3)}s\")\n",
    "    t0 = time()\n",
    "    score_train = model.score(x_train, y_train)\n",
    "    print(f\"Prediction time (train): {round(time()-t0, 3)}s\")\n",
    "    t0 = time()\n",
    "    score_test = model.score(x_test, y_test)\n",
    "    print(f\"Prediction time (test): {round(time()-t0, 3)}s\")\n",
    "    print(\"\\nTrain set score:\", score_train)\n",
    "    print(\"Test set score:\", score_test)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "def train_data_svm(x_train, x_test, y_train, y_test):\n",
    "    t0 = time()\n",
    "    model = svm.SVC(kernel='linear')\n",
    "    model.fit(x_train, y_train)\n",
    "    print(f\"\\nTraining time: {round(time()-t0, 3)}s\")\n",
    "    t0 = time()\n",
    "    score_train = model.score(x_train, y_train)\n",
    "    print(f\"Prediction time (train): {round(time()-t0, 3)}s\")\n",
    "    t0 = time()\n",
    "    score_test = model.score(x_test, y_test)\n",
    "    print(f\"Prediction time (test): {round(time()-t0, 3)}s\")\n",
    "    print(\"\\nTrain set score:\", score_train)\n",
    "    print(\"Test set score:\", score_test)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(text, model):\n",
    "    data = [text]\n",
    "    vec_data = vectorizer.transform(data).toarray()\n",
    "    return model.predict(vec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def evaluate_model(model, vectorized_test_data, label):\n",
    "    x_test = vectorized_test_data\n",
    "    y_pred = model.predict(x_test)\n",
    "    return precision_recall_fscore_support(label, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Program!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = read_data(\"Data skripsi/fix hasil label/aspek_labeling_1.csv\")\n",
    "df = read_data(\"Data skripsi/dataset_ulasan_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#aspects: 'Learnability', 'Efficiency', 'Memorability', 'Errors', 'Satisfaction'\n",
    "#Pelabelan dengan 1 dan 0\n",
    "\n",
    "col_name = 'Learnability'\n",
    "df_sentimen = convert_sentiment_label(df, col_name)\n",
    "df_aspect = convert_aspect_label(df, col_name) \n",
    "visualize_data(df_sentimen, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Data \n",
    "\n",
    "x_train1, x_test1, y_train1, y_test1 = split_data(df_aspect, col_name, test_size= 0.2) #Split Data untuk klasifikasi aspek\n",
    "x_train2, x_test2, y_train2, y_test2 = split_data(df_sentimen, col_name, test_size= 0.2) #Split Data untuk klasifikasi sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing 1\n",
    "\n",
    "x_train1 = [preprocess1(text) for text in x_train1]\n",
    "x_test1 = [preprocess1(text) for text in x_test1]\n",
    "\n",
    "x_train2 = [preprocess1(text) for text in x_train2]\n",
    "x_test2 = [preprocess1(text) for text in x_test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pos Tagging untuk data ulasan klasifikasi sentimen\n",
    "\n",
    "x_train2 = pos_tagging(x_train2)\n",
    "x_test2 = pos_tagging(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess 2\n",
    "\n",
    "x_train1 = [preprocess2(text) for text in x_train1]\n",
    "x_test1 = [preprocess2(text) for text in x_test1]\n",
    "\n",
    "x_train2 = [preprocess2(text) for text in x_train2]\n",
    "x_test2 = [preprocess2(text) for text in x_test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(tokens):\n",
    "    return TreebankWordDetokenizer().detokenize(tokens)\n",
    "\n",
    "x_train1 = [detokenize(text) for text in x_train1]\n",
    "x_test1 = [detokenize(text) for text in x_test1]\n",
    "\n",
    "x_train2 = [detokenize(text) for text in x_train2]\n",
    "x_test2 = [detokenize(text) for text in x_test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ekstraksi fitur TF-IDF\n",
    "\n",
    "vx_train1, vx_test1, vectorizer1 = tfidf_vectorizer(x_train1, x_test1) #Untuk klasifikasi aspek\n",
    "vx_train2, vx_test2, vectorizer2 = tfidf_vectorizer(x_train2, x_test2) #Untuk klasifikasi sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 2537, 1: 496})\n",
      "Resampled dataset shape Counter({0: 2537, 1: 2537})\n",
      "----------------------------------------------------\n",
      "Original dataset shape Counter({0: 260, 1: 233})\n",
      "Resampled dataset shape Counter({1: 260, 0: 260})\n"
     ]
    }
   ],
   "source": [
    "#Oversampling\n",
    "\n",
    "vx_train1, y_train1 = apply_oversampling(vx_train1, y_train1) #Oversampling data untuk klasifikasi aspek\n",
    "print('----------------------------------------------------')\n",
    "vx_train2, y_train2 = apply_oversampling(vx_train2, y_train2) ##Oversampling data untuk klasifikasi aspek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 0.101s\n",
      "Prediction time (train): 0.1s\n",
      "Prediction time (test): 0.025s\n",
      "\n",
      "Train set score: 0.9020496649586125\n",
      "Test set score: 0.6205533596837944\n",
      "--------------------------------------------------------\n",
      "\n",
      "Training time: 0.004s\n",
      "Prediction time (train): 0.003s\n",
      "Prediction time (test): 0.001s\n",
      "\n",
      "Train set score: 0.9346153846153846\n",
      "Test set score: 0.6209677419354839\n"
     ]
    }
   ],
   "source": [
    "#Model Training Multi-NB\n",
    "\n",
    "model_nb1 = train_data_nb(vx_train1, vx_test1, y_train1, y_test1) #Training Model klasifikasi aspek \n",
    "print('--------------------------------------------------------')\n",
    "model_nb2 = train_data_nb(vx_train2, vx_test2, y_train2, y_test2) #Training Model klasifikasi sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6205533596837944, 0.6205533596837944, 0.6205533596837944, None)\n",
      "-----------------------------------------\n",
      "(0.6209677419354839, 0.6209677419354839, 0.6209677419354839, None)\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_model(model_nb1, vx_test1, y_test1))\n",
    "print('-----------------------------------------')\n",
    "print(evaluate_model(model_nb2, vx_test2, y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 0.382s\n",
      "Prediction time (train): 0.275s\n",
      "Prediction time (test): 0.067s\n",
      "\n",
      "Train set score: 1.0\n",
      "Test set score: 0.9112903225806451\n"
     ]
    }
   ],
   "source": [
    "#Model Training SVM\n",
    "\n",
    "model_svm1 = train_data_svm(vx_train1, vx_test1, y_train1, y_test1) #Training Model klasifikasi aspek \n",
    "print('----------------------------------------------------------')\n",
    "model_svm2 = train_data_svm(vx_train2, vx_test2, y_train2, y_test2) #Training Model klasifikasi sentimen \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9112903225806451, 0.9112903225806451, 0.9112903225806451, None)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model_svm, vx_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"bagus\"\n",
    "predict_data(text, model_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SOurce: https://towardsdatascience.com/training-a-naive-bayes-model-to-identify-the-author-of-an-email-or-document-17dc85fa630a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
